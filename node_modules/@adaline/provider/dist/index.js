'use strict';

var types = require('@adaline/types');
var zod = require('zod');

var Y=Object.defineProperty;var O=Object.getOwnPropertySymbols;var X=Object.prototype.hasOwnProperty,$=Object.prototype.propertyIsEnumerable;var R=(e,n,t)=>n in e?Y(e,n,{enumerable:!0,configurable:!0,writable:!0,value:t}):e[n]=t,a=(e,n)=>{for(var t in n||(n={}))X.call(n,t)&&R(e,t,n[t]);if(O)for(var t of O(n))$.call(n,t)&&R(e,t,n[t]);return e};var v="ProviderError",j=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},v);this.name=v;this.info=t,this.cause=o;}static isProviderError(t){return t instanceof e}};var P="ModelError",z=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},P);this.name=P;this.info=t,this.cause=o;}static isModelError(t){return t instanceof e}};var D="ModelResponseError",B=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},D);this.name=D;this.cause=o,this.info=t;}static isModelResponseError(t){return t instanceof e}};var A="InvalidModelRequestError",N=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},A);this.name=A;this.cause=o,this.info=t,Object.setPrototypeOf(this,new.target.prototype);}static isInvalidModelRequestError(t){return t instanceof e}};var L="InvalidConfigError",_=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},L);this.name=L;this.cause=o,this.info=t,Object.setPrototypeOf(this,new.target.prototype);}static isInvalidConfigError(t){return t instanceof e}};var G="InvalidMessagesError",U=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},G);this.name=G;this.cause=o,this.info=t,Object.setPrototypeOf(this,new.target.prototype);}static isInvalidMessagesError(t){return t instanceof e}};var q="InvalidToolsError",Z=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},q);this.name=q;this.cause=o,this.info=t,Object.setPrototypeOf(this,new.target.prototype);}static isInvalidToolsError(t){return t instanceof e}};var F="InvalidEmbeddingRequestsError",K=class e extends types.GatewayBaseError{constructor({info:t,cause:o}){super({info:t,cause:o},F);this.name=F;this.info=t,this.cause=o,Object.setPrototypeOf(this,new.target.prototype);}static isInvalidEmbeddingRequestsError(t){return t instanceof e}};var y="multi-string",C=zod.z.object({type:zod.z.literal(y),param:zod.z.string().min(1),title:zod.z.string().min(1),description:zod.z.string().min(1).max(500),max:zod.z.number().int().positive()}),se=e=>zod.z.array(zod.z.string()).max(e).default([]).optional(),et=e=>({def:C.parse(a({type:y},e)),schema:se(e.max)});var h="object-schema",x=zod.z.object({type:zod.z.literal(h),param:zod.z.string().min(1),title:zod.z.string().min(1),description:zod.z.string().min(1).max(500),objectSchema:zod.z.any()}),ae=e=>e.optional(),it=e=>({def:x.parse(a({type:h},e)),schema:ae(e.objectSchema)});var S="range",w=zod.z.object({type:zod.z.literal(S),param:zod.z.string().min(1),title:zod.z.string().min(1),description:zod.z.string().min(1).max(500),min:zod.z.number().int(),max:zod.z.number().int(),step:zod.z.number().positive(),default:zod.z.number()}),ce=(e,n,t,o)=>zod.z.number().min(e).max(n).step(t).default(o).optional(),ct=e=>({def:w.parse(a({type:S},e)),schema:ce(e.min,e.max,e.step,e.default)});var b="select-boolean",k=zod.z.object({type:zod.z.literal(b),param:zod.z.string().min(1),title:zod.z.string().min(1),description:zod.z.string().min(1).max(500),default:zod.z.boolean().nullable()}),me=e=>zod.z.boolean().nullable().default(e).optional(),ft=e=>({def:k.parse(a({type:b},e)),schema:me(e.default)});var T="select-string",M=zod.z.object({type:zod.z.literal(T),param:zod.z.string().min(1),title:zod.z.string().min(1),description:zod.z.string().min(1).max(500),default:zod.z.string(),choices:zod.z.array(zod.z.string())}),pe=(e,n)=>zod.z.enum(n).nullable().default(e).optional(),yt=e=>({def:M.parse(a({type:T},e)),schema:pe(e.default,e.choices)});var le=[S,y,T,h,b],wt=zod.z.enum(le),I=zod.z.discriminatedUnion("type",[w,C,M,k,x]);var jt=(e=types.RoleEnum,n=types.ModalityEnum)=>zod.z.object({name:zod.z.string().min(1),description:zod.z.string().min(1),roles:zod.z.record(e,zod.z.string().min(1).optional()),modalities:zod.z.array(n).nonempty(),maxInputTokens:zod.z.number().int().positive().min(1),maxOutputTokens:zod.z.number().int().positive().min(1),maxReasoningTokens:zod.z.number().int().positive().min(1).optional(),config:zod.z.object({def:zod.z.record(zod.z.string().min(1),I),schema:zod.z.instanceof(zod.z.ZodObject)}).refine(t=>{var d,g;let o=Object.keys(t.def),s=Object.keys((g=(d=t.schema)==null?void 0:d.shape)!=null?g:{});return o.every(E=>s.includes(E))&&s.every(E=>o.includes(E))},{message:"Keys in 'config.def' must exactly match keys in 'config.schema'"}),price:zod.z.custom()});var At=(e=types.EmbeddingModalityEnum)=>zod.z.object({name:zod.z.string().min(1),description:zod.z.string().min(1),modalities:zod.z.array(e).nonempty(),maxInputTokens:zod.z.number().int().positive().min(1),maxOutputTokens:zod.z.number().int().positive().min(1),config:zod.z.object({def:zod.z.record(zod.z.string().min(1),I),schema:zod.z.instanceof(zod.z.ZodObject)}).refine(n=>{var s,d;let t=Object.keys(n.def),o=Object.keys((d=(s=n.schema)==null?void 0:s.shape)!=null?d:{});return t.every(g=>o.includes(g))&&o.every(g=>t.includes(g))},{message:"Keys in 'config.def' must exactly match keys in 'config.schema'"})});var _t=zod.z.record(zod.z.string());var qt=zod.z.record(zod.z.union([zod.z.boolean(),zod.z.string(),zod.z.number(),zod.z.object({}),zod.z.array(zod.z.any()),zod.z.null(),zod.z.undefined()]));var Kt=zod.z.string().url();var ye={type:"range",title:"Temperature",description:"Adjusts the model's creativity level. With a setting of 0, the model strictly picks the most probable next word.     For endeavors that benefit from a dash of inventiveness, consider dialing it up to 0.7 or higher, enabling the model to produce text     that's unexpectedly fresh."},he={type:"range",title:"Max tokens",description:"Specify the total tokens for generation, where one token approximates four English characters.     Setting this to 0 defaults to the model's maximum capacity."},Se={type:"range",title:"Max reasoning tokens",description:"Specify the total tokens for reasoning, where one token approximates four English characters."},be=e=>({type:"multi",title:"Stop sequence",description:`Enter up to ${e} sequences that will halt additional text output.       The generated text will exclude these sequences.`}),Te={type:"range",title:"Top A",description:"Considers only the top tokens that have 'sufficiently high' probabilities relative to the most likely token,     functioning like a dynamic Top-P.     A lower Top-A value narrows down the token choices based on the highest probability token,     while a higher Top-A value refines the filtering without necessarily impacting the creativity of the output."},Ie={type:"range",title:"Top P",description:"Selects a subset of likely tokens for generation, restricting choices to the top-P fraction of possibilities,     such as the top 10% when P=0.1.     This approach can limit the variety of the output. By default, it's set to 1, indicating no restriction.     It's advised to adjust this parameter or temperature to modulate output diversity, but not to modify both simultaneously."},Ee={type:"range",title:"Top K",description:"Select only from the highest K probabilities for each following word, effectively eliminating the less likely 'long tail' options."},Ce={type:"range",title:"Min P",description:"Specifies the minimum probability a token must have to be considered, in relation to the probability of the most likely token.     (This value varies based on the confidence level of the top token.)     For example, if Min-P is set to 0.1, only tokens with at least 1/10th the probability of the highest-ranked token will be considered."},xe={type:"range",title:"Frequency penalty",description:"Minimize redundancy.    By assigning a penalty to frequently used tokens within the text, the likelihood of repeating identical phrases is reduced.     The default setting for this penalty is zero."},we={type:"range",title:"Presence penalty",description:"Enhance the introduction of novel subjects by reducing the preference for tokens that have already appeared in the text,     thus boosting the chances of exploring fresh topics.     The standard setting for this is zero."},ke={type:"range",title:"Seed",description:"When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests.     Deterministic output isn't guaranteed.     Also, changing the model or parameter settings, such as the temperature,     can cause variations in the response even when you use the same seed value.     By default, a random seed value is used."},Me={type:"range",title:"Repetition penalty",description:"Reduces the likelihood of repeating tokens from the input.     Increasing this value makes the model less prone to repetition, but setting it too high may lead to less coherent output,     often resulting in run-on sentences missing smaller words.     The token penalty is scaled according to the original token's probability."},Oe={type:"boolean",title:"Log probs",description:"Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned."},Re={type:"range",title:"Top log probs",description:"The number of most likely tokens to return at each token position, each with an associated log probability.     'logprobs' must be set to true if this parameter is used."},ve={type:"boolean",title:"Echo",description:"If true, the response will contain the prompt."},je={type:"select",title:"Response format",description:"Choose the response format of your model. For JSON, you must include the string 'JSON' in some form within your system / user prompt."},Pe={type:"select",title:"Response format",description:"Choose the response format of your model. 'json_object' colloquially known as JSON mode, instructs the model to respond with a valid   JSON (must include the term 'json' in prompt). 'json_schema' colloquially known as structured outputs, allows you to specify a strict   response schema that the model will adhere to."},ze={type:"object",title:"Response schema",description:"When response format is set to 'json_schema', the model will return a JSON object of the specified schema."},Jt={TEMPERATURE:ye,MAX_TOKENS:he,STOP:be,TOP_A:Te,TOP_P:Ie,TOP_K:Ee,MIN_P:Ce,FREQUENCY_PENALTY:xe,PRESENCE_PENALTY:we,REPETITION_PENALTY:Me,SEED:ke,LOG_PROBS:Oe,TOP_LOG_PROBS:Re,ECHO:ve,RESPONSE_FORMAT:je,RESPONSE_FORMAT_WITH_SCHEMA:Pe,RESPONSE_SCHEMA:ze,MAX_REASONING_TOKENS:Se};var De={type:"range",title:"Dimensions",description:"Select the number of dimensions for the word embedding."},Be={type:"select",title:"Encoding format",description:"Select the encoding format for the word embedding."},Yt={DIMENSIONS:De,ENCODING_FORMAT:Be};var $t=e=>e==null?"unknown error":typeof e=="string"?e:e instanceof Error?e.message:JSON.stringify(e);var Vt=e=>Object.fromEntries(Object.entries(e).filter(([n,t])=>t!=null));var W=()=>typeof window!="undefined"&&typeof window.document!="undefined"&&typeof navigator!="undefined";var on=e=>{let n=e.replace(/-/g,"+").replace(/_/g,"/"),t=globalThis.atob(n);return Uint8Array.from(t,o=>o.codePointAt(0))},rn=e=>{let n="";for(let t=0;t<e.length;t++)n+=String.fromCodePoint(e[t]);return globalThis.btoa(n)},sn=e=>{if(W()){let n=atob(e),t=new Uint8Array(n.length);for(let s=0;s<n.length;s++)t[s]=n.charCodeAt(s);return new TextDecoder("utf-8").decode(t)}else return Buffer.from(e,"base64").toString("utf-8")},an=e=>e.split(";")[0].split("/")[1];var mn=e=>e==null?void 0:e.replace(/\/$/,"");

exports.CHAT_CONFIG = Jt;
exports.ChatModelSchema = jt;
exports.ConfigItemDef = I;
exports.ConfigItemEnum = wt;
exports.ConfigItemLiterals = le;
exports.EMBEDDING_CONFIG = Yt;
exports.EmbeddingModelSchema = At;
exports.Headers = _t;
exports.InvalidConfigError = _;
exports.InvalidEmbeddingRequestsError = K;
exports.InvalidMessagesError = U;
exports.InvalidModelRequestError = N;
exports.InvalidToolsError = Z;
exports.ModelError = z;
exports.ModelResponseError = B;
exports.MultiStringConfigItem = et;
exports.MultiStringConfigItemDef = C;
exports.MultiStringConfigItemSchema = se;
exports.MultiStringConfigItemTypeLiteral = y;
exports.ObjectSchemaConfigItem = it;
exports.ObjectSchemaConfigItemDef = x;
exports.ObjectSchemaConfigItemSchema = ae;
exports.ObjectSchemaConfigItemTypeLiteral = h;
exports.Params = qt;
exports.ProviderError = j;
exports.RangeConfigItem = ct;
exports.RangeConfigItemDef = w;
exports.RangeConfigItemSchema = ce;
exports.RangeConfigItemTypeLiteral = S;
exports.SelectBooleanConfigItem = ft;
exports.SelectBooleanConfigItemDef = k;
exports.SelectBooleanConfigItemSchema = me;
exports.SelectBooleanConfigItemTypeLiteral = b;
exports.SelectStringConfigItem = yt;
exports.SelectStringConfigItemDef = M;
exports.SelectStringConfigItemSchema = pe;
exports.SelectStringConfigItemTypeLiteral = T;
exports.Url = Kt;
exports.convertBase64ToUint8Array = on;
exports.convertUint8ArrayToBase64 = rn;
exports.encodedBase64ToString = sn;
exports.getErrorMessage = $t;
exports.getMimeTypeFromBase64 = an;
exports.isRunningInBrowser = W;
exports.removeUndefinedEntries = Vt;
exports.urlWithoutTrailingSlash = mn;
//# sourceMappingURL=index.js.map
//# sourceMappingURL=index.js.map