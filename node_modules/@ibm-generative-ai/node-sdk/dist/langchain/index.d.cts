import { BaseLLMParams, BaseLLMCallOptions, BaseLLM } from '@langchain/core/language_models/llms';
import { CallbackManagerForLLMRun } from '@langchain/core/callbacks/manager';
import { LLMResult, GenerationChunk, ChatResult, ChatGenerationChunk } from '@langchain/core/outputs';
import { Serialized } from '@langchain/core/load/serializable';
import { al as Client, aj as Configuration, T as TextGenerationCreateInput, b as TextGenerationCreateStreamInput, f as TextChatCreateInput, h as TextChatCreateStreamInput } from '../client-AWX42XP4.cjs';
import { BaseChatModelParams, BaseChatModel } from '@langchain/core/language_models/chat_models';
import { BaseMessage } from '@langchain/core/messages';
import { BaseLanguageModelCallOptions } from '@langchain/core/language_models/base';
import { PromptTemplate } from '@langchain/core/prompts';
import 'openapi-fetch';
import 'stream';
import 'p-queue-compat';

type TextGenerationInput = TextGenerationCreateInput & TextGenerationCreateStreamInput;
type GenAIModelParams = BaseLLMParams & Pick<TextGenerationInput, 'model_id' | 'prompt_id' | 'parameters' | 'moderations'> & {
    model_id: NonNullable<TextGenerationInput['model_id']>;
} & ({
    client: Client;
    configuration?: never;
} | {
    client?: never;
    configuration: Configuration;
});
type GenAIModelOptions = BaseLLMCallOptions & Partial<Omit<GenAIModelParams, 'client' | 'configuration'>>;
declare class GenAIModel extends BaseLLM<GenAIModelOptions> {
    readonly client: Client;
    readonly modelId: GenAIModelParams['model_id'];
    readonly promptId: GenAIModelParams['prompt_id'];
    readonly parameters: GenAIModelParams['parameters'];
    readonly moderations: GenAIModelParams['moderations'];
    constructor({ model_id, prompt_id, parameters, moderations, client, configuration, ...options }: GenAIModelParams);
    _generate(inputs: string[], options: this['ParsedCallOptions'], runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;
    _streamResponseChunks(input: string, options: this['ParsedCallOptions'], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;
    private _prepareRequest;
    getNumTokens(input: string): Promise<number>;
    static fromJSON(value: string | Serialized, client?: Client): Promise<unknown>;
    _modelType(): string;
    _llmType(): string;
    lc_serializable: boolean;
    lc_namespace: string[];
    get lc_id(): string[];
    lc_kwargs: {
        modelId: undefined;
        promptId: undefined;
        parameters: undefined;
        moderations: undefined;
        client: undefined;
    };
    get lc_secrets(): {
        client: string;
    };
}

type TextChatInput = TextChatCreateInput & TextChatCreateStreamInput;
type GenAIChatModelParams = BaseChatModelParams & Omit<TextChatInput, 'messages' | 'prompt_template_id'> & {
    model_id: NonNullable<TextChatInput['model_id']>;
} & ({
    client: Client;
    configuration?: never;
} | {
    client?: never;
    configuration: Configuration;
});
type GenAIChatModelOptions = BaseLanguageModelCallOptions & Partial<Omit<GenAIChatModelParams, 'client' | 'configuration'>>;
declare class GenAIChatModel extends BaseChatModel<GenAIChatModelOptions> {
    readonly client: Client;
    readonly modelId: GenAIChatModelParams['model_id'];
    readonly promptId: GenAIChatModelParams['prompt_id'];
    readonly conversationId: GenAIChatModelParams['conversation_id'];
    readonly parameters: GenAIChatModelParams['parameters'];
    readonly moderations: GenAIChatModelParams['moderations'];
    readonly useConversationParameters: GenAIChatModelParams['use_conversation_parameters'];
    readonly parentId: GenAIChatModelParams['parent_id'];
    readonly trimMethod: GenAIChatModelParams['trim_method'];
    constructor({ model_id, prompt_id, conversation_id, parameters, moderations, parent_id, use_conversation_parameters, trim_method, client, configuration, ...options }: GenAIChatModelParams);
    _generate(messages: BaseMessage[], options: this['ParsedCallOptions'], _runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _streamResponseChunks(messages: BaseMessage[], options: this['ParsedCallOptions'], _runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    private _prepareRequest;
    private _convertMessages;
    lc_serializable: boolean;
    lc_namespace: string[];
    get lc_id(): string[];
    lc_kwargs: {
        modelId: undefined;
        promptId: undefined;
        conversationId: undefined;
        parameters: undefined;
        moderations: undefined;
        useConversationParameters: undefined;
        parentId: undefined;
        trimMethod: undefined;
        client: undefined;
    };
    get lc_secrets(): {
        client: string;
    };
    static fromJSON(value: string | Serialized, client?: Client): Promise<unknown>;
    _modelType(): string;
    _llmType(): string;
}

declare class GenAIPromptTemplate {
    static toLangChain(body: string): PromptTemplate;
    static fromLangChain(template: PromptTemplate): string;
    private static getTemplateMatcher;
}

export { GenAIChatModel, type GenAIChatModelOptions, type GenAIChatModelParams, GenAIModel, type GenAIModelOptions, type GenAIModelParams, GenAIPromptTemplate };
